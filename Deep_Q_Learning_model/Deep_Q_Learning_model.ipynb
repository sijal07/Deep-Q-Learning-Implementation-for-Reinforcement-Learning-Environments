{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SF5mNJ_IXldb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8a42f10"
      },
      "source": [
        "# Task\n",
        "Build a Deep Q-Learning model using PyTorch to solve the Lunar Landing environment and generate a video of the trained agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "769ea7c3"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install PyTorch, Gymnasium (for the LunarLander-v2 environment), and other required libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2537a648"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the necessary libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3364edd6",
        "outputId": "14cf45a5-e3c3-43cc-96eb-cdf022749e66"
      },
      "source": [
        "!pip install torch torchvision gymnasium gymnasium[box2d] moviepy tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51c4baed"
      },
      "source": [
        "**Reasoning**:\n",
        "The installation of box2d-py failed. Attempt to install it separately with a specific version and potentially resolve build issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48ee14b6",
        "outputId": "56f74093-715d-469f-d649-5b8ca6351b29"
      },
      "source": [
        "!pip install box2d-py==2.3.5 --no-build-isolation"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py==2.3.5\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ee834c6"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install PyTorch, Gymnasium (for the LunarLander-v2 environment), and other required libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae509970"
      },
      "source": [
        "**Reasoning**:\n",
        "Attempt to install swig and then box2d-py again as suggested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a693c01",
        "outputId": "5098700c-fa79-4cfd-989e-40d2054089c5"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install swig -y\n",
        "!pip install box2d-py==2.3.5 --no-build-isolation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [1 InRelease 12.7 kB/129 k\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connected to cloud.r-proj\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [2 InRelease 3,632 B/3,632 B 100%] [Connected to r2u.s\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rHit:3 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rHit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r                                                                               \r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)]\r                                                                              \rHit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                              \rHit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,275 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,415 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,065 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,580 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,323 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,744 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,922 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,809 kB]\n",
            "Fetched 30.5 MB in 3s (8,843 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,429 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting box2d-py==2.3.5\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp312-cp312-linux_x86_64.whl size=2381958 sha256=ee002b36d5a8dc3f45fbcd8b6b500c1897be2b2aa4a8ccfd900dda583dd83dbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/e9/60/774da0bcd07f7dc7761a8590fa2d065e4069568e78dcdc3318\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7de3c953"
      },
      "source": [
        "## Set up the environment\n",
        "\n",
        "### Subtask:\n",
        "Create the LunarLander-v2 environment using Gymnasium.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dedc9c72"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the gymnasium library and create the LunarLander-v2 environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9948dcf9"
      },
      "source": [
        "**Reasoning**:\n",
        "The LunarLander-v2 environment is deprecated. Create the LunarLander-v3 environment instead.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3095710"
      },
      "source": [
        "## How to Create a README File\n",
        "\n",
        "A README file is typically a plain text file named `README.md` (using Markdown format) located in the root directory of your project. It serves as the first point of contact for anyone looking at your code.\n",
        "\n",
        "Here's a suggested structure for your README:\n",
        "\n",
        "1.  **Title:** The name of your project.\n",
        "2.  **Description:** A brief explanation of what your project does and its purpose. For this project, you can describe that it's a Deep Q-Learning model for the Lunar Lander environment.\n",
        "3.  **Installation:** Instructions on how to install the necessary dependencies. You can list the libraries you installed (`torch`, `gymnasium`, `gymnasium[box2d]`, `moviepy`, `tqdm`) and mention the `pip` commands used.\n",
        "4.  **Usage:** How to run your code. You can explain the different sections of your notebook: setting up the environment, defining the network and agent, training, evaluating, and generating the video. You can also mention how to load the saved model checkpoint.\n",
        "5.  **Results:** Briefly summarize the training results (e.g., average score achieved, number of episodes to solve the environment) and mention the generated video.\n",
        "6.  **Contributing (Optional):** How others can contribute to your project.\n",
        "7.  **License (Optional):** The license under which your project is released.\n",
        "\n",
        "You can create this file in your project directory using a text editor and write the content using Markdown syntax. Markdown is a lightweight markup language that allows you to format plain text (like using `#` for headings, `*` or `-` for lists, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bloDia18YTbY"
      },
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('LunarLander-v3')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc3cc005"
      },
      "source": [
        "## Define the q-network\n",
        "\n",
        "### Subtask:\n",
        "Implement a neural network using PyTorch that will approximate the Q-values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "089488ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the neural network for the Q-Network using PyTorch as described in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d131938d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    \"\"\"Deep neural network for Q-value approximation.\"\"\"\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \"\"\"Initialize parameters and build network.\n",
        "\n",
        "        Args:\n",
        "            state_size (int): Dimension of each state.\n",
        "            action_size (int): Dimension of each action.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Example usage (optional, for verification)\n",
        "# state_size = env.observation_space.shape[0]\n",
        "# action_size = env.action_space.n\n",
        "# q_network = QNetwork(state_size, action_size)\n",
        "# print(q_network)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14dad2e8"
      },
      "source": [
        "## Implement the deep q-learning agent\n",
        "\n",
        "### Subtask:\n",
        "Create an agent class that includes methods for selecting actions (epsilon-greedy), storing experiences in a replay buffer, and training the Q-Network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a5a7f5f"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the DQNAgent class with the specified methods for action selection, experience storage, and learning. This involves initializing the Q-networks, replay buffer, and optimizer, as well as defining the `act`, `step`, `learn`, and `soft_update` methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fd50614"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR = 5e-4               # learning rate\n",
        "UPDATE_EVERY = 4        # how often to update the network\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA, tau=TAU, lr=LR, update_every=UPDATE_EVERY):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Args:\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "            buffer_size (int): replay buffer size\n",
        "            batch_size (int): minibatch size\n",
        "            gamma (float): discount factor\n",
        "            tau (float): for soft update of target parameters\n",
        "            lr (float): learning rate\n",
        "            update_every (int): how often to update the network\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.lr = lr\n",
        "        self.update_every = update_every\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.lr)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(buffer_size, batch_size, seed)\n",
        "        # Initialize time step (for updating every 'update_every' steps)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > self.batch_size:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, self.gamma)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        \"\"\"Returns actions for given state as per epsilon-greedy policy.\n",
        "\n",
        "        Args:\n",
        "            state (np.array): current state\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"Update value parameters using given batch of experience tuples.\n",
        "\n",
        "        Args:\n",
        "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Get max predicted Q values (for next states) from target model\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        # Compute Q targets for current states\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        # Get expected Q values from local model\n",
        "        Q_expecteds = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = torch.nn.functional.mse_loss(Q_expecteds, Q_targets)\n",
        "        # Minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Soft update target network\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "\n",
        "        Args:\n",
        "            local_model (torch.nn.Module): weights will be copied from\n",
        "            target_model (torch.nn.Module): weights will be copied to\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Args:\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "# Example usage (optional, for verification)\n",
        "# state_size = env.observation_space.shape[0]\n",
        "# action_size = env.action_space.n\n",
        "# agent = DQNAgent(state_size=state_size, action_size=action_size, seed=0)\n",
        "# print(agent)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09740f7d"
      },
      "source": [
        "## Train the agent\n",
        "\n",
        "### Subtask:\n",
        "Implement the training loop, including steps for interacting with the environment, storing experiences, and updating the Q-Network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73c5835f"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `dqn` function to handle the training loop as described in the instructions, including iterating through episodes and timesteps, interacting with the environment, storing experiences, updating the agent, and tracking scores. After defining the function, call it with the specified parameters and store the returned scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbe90257",
        "outputId": "71bd6c9f-bd72-4a3c-dd23-642783206a32"
      },
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, scores_window=100):\n",
        "    \"\"\"Deep Q-Learning.\n",
        "\n",
        "    Args:\n",
        "        n_episodes (int): maximum number of training episodes\n",
        "        max_t (int): maximum number of timesteps per episode\n",
        "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "        eps_end (float): minimum value of epsilon\n",
        "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "        scores_window (int): window size for calculating the average score\n",
        "    \"\"\"\n",
        "    scores = []                         # list containing scores from each episode\n",
        "    scores_window_deque = deque(maxlen=scores_window)  # last 100 scores\n",
        "    eps = eps_start                     # initialize epsilon\n",
        "\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size=state_size, action_size=action_size, seed=0)\n",
        "\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "        scores_window_deque.append(score)       # save most recent score\n",
        "        scores.append(score)                  # save most recent score\n",
        "        avg_score = np.mean(scores_window_deque)\n",
        "\n",
        "        print(f'\\rEpisode {i_episode}\\tAverage Score: {avg_score:.2f}\\tEpsilon: {eps:.2f}', end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print(f'\\rEpisode {i_episode}\\tAverage Score: {avg_score:.2f}\\tEpsilon: {eps:.2f}')\n",
        "\n",
        "        eps = max(eps_end, eps_decay * eps) # decrease epsilon\n",
        "\n",
        "        if avg_score >= 200.0:\n",
        "            print(f'\\nEnvironment solved in {i_episode:d} episodes!\\tAverage Score: {avg_score:.2f}')\n",
        "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "            break\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = dqn(n_episodes=2000, max_t=1000, eps_end=0.01, eps_decay=0.995)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: -154.62\tEpsilon: 0.61\n",
            "Episode 200\tAverage Score: -140.39\tEpsilon: 0.37\n",
            "Episode 300\tAverage Score: -65.22\tEpsilon: 0.22\n",
            "Episode 400\tAverage Score: -9.69\tEpsilon: 0.14\n",
            "Episode 500\tAverage Score: 34.86\tEpsilon: 0.08\n",
            "Episode 600\tAverage Score: 107.42\tEpsilon: 0.05\n",
            "Episode 700\tAverage Score: 175.52\tEpsilon: 0.03\n",
            "Episode 770\tAverage Score: 200.86\tEpsilon: 0.02\n",
            "Environment solved in 770 episodes!\tAverage Score: 200.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "624b19d5"
      },
      "source": [
        "## Evaluate the trained agent\n",
        "\n",
        "### Subtask:\n",
        "Run the trained agent in the environment for a few episodes to see how it performs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39822c0d"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the trained model weights, set the agent to evaluation mode, and run the environment for a few episodes using the trained agent with no exploration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2971cdf",
        "outputId": "9b8c8e3e-b7c7-4b41-83da-686134697248"
      },
      "source": [
        "# Load the trained weights from the checkpoint\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size=state_size, action_size=action_size, seed=0)\n",
        "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
        "\n",
        "# Set the agent to evaluation mode\n",
        "agent.qnetwork_local.eval()\n",
        "\n",
        "# Run the environment for a few episodes\n",
        "num_episodes_to_run = 5  # You can adjust this number\n",
        "for i_episode in range(1, num_episodes_to_run + 1):\n",
        "    state, _ = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        # Select an action using the agent's act method with eps=0 (no exploration)\n",
        "        action = agent.act(state, eps=0)\n",
        "        # Step the environment with the selected action\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        # Accumulate the reward\n",
        "        score += reward\n",
        "        # Update the current state\n",
        "        state = next_state\n",
        "        # If the episode is done, break the inner loop\n",
        "        if done:\n",
        "            break\n",
        "    # Print the total score for the episode\n",
        "    print(f'Episode {i_episode}\\tScore: {score:.2f}')\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1\tScore: 232.87\n",
            "Episode 2\tScore: 272.17\n",
            "Episode 3\tScore: 252.44\n",
            "Episode 4\tScore: 247.71\n",
            "Episode 5\tScore: 232.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06185426"
      },
      "source": [
        "## Generate video of the trained agent\n",
        "\n",
        "### Subtask:\n",
        "Record a video of the trained agent interacting with the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c527b59b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires recording a video of the trained agent. This involves creating a new environment instance with a specific render mode, wrapping it with a video recorder, loading the trained agent, running the simulation, and closing the environment. These steps can be combined into a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c3f0d3b",
        "outputId": "5d5f85dc-7e7a-4d08-a63f-efdbe2b6e359"
      },
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# 1. Create a new LunarLander-v3 environment instance with render_mode='rgb_array'.\n",
        "# 2. Wrap the environment with the gymnasium.wrappers.RecordVideo wrapper.\n",
        "video_dir = 'videos'\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "eval_env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
        "eval_env = gym.wrappers.RecordVideo(eval_env, video_folder=video_dir, episode_trigger=lambda x: x == 0) # Record the first episode\n",
        "\n",
        "# 3. Load the trained agent's state dictionary from 'checkpoint.pth'.\n",
        "state_size = eval_env.observation_space.shape[0]\n",
        "action_size = eval_env.action_space.n\n",
        "agent = DQNAgent(state_size=state_size, action_size=action_size, seed=0)\n",
        "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
        "\n",
        "# 4. Set the agent to evaluation mode.\n",
        "agent.qnetwork_local.eval()\n",
        "\n",
        "# 5. Run the wrapped environment for one episode.\n",
        "state, _ = eval_env.reset()\n",
        "score = 0\n",
        "while True:\n",
        "    action = agent.act(state, eps=0) # Use eps=0 for evaluation\n",
        "    next_state, reward, done, _, _ = eval_env.step(action)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(f'Episode finished with score: {score:.2f}')\n",
        "\n",
        "# 6. Close the environment after the episode is finished to ensure the video is saved.\n",
        "eval_env.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished with score: 295.51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934f4079"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial installation of `box2d-py`, a dependency for the Lunar Lander environment, failed due to build errors, which were resolved by installing `swig` beforehand.\n",
        "*   The `LunarLander-v2` environment is deprecated, and the analysis successfully used the recommended `LunarLander-v3` environment.\n",
        "*   A `QNetwork` class was successfully implemented in PyTorch for approximating Q-values.\n",
        "*   A `DQNAgent` class and a `ReplayBuffer` class were implemented to handle action selection (epsilon-greedy), experience storage, and Q-network training.\n",
        "*   The training loop successfully trained the agent to achieve an average score of 200.86 over 100 episodes within 770 episodes, solving the Lunar Lander environment.\n",
        "*   The trained agent, when evaluated for 5 episodes, consistently achieved positive scores ranging from approximately 232 to 272, demonstrating effective learning.\n",
        "*   A video of the trained agent's performance on one episode was successfully recorded, showing a score of 295.51.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The training process was effective in solving the Lunar Lander environment, indicated by the high evaluation scores and the successful recording of a high-scoring episode video.\n",
        "*   The saved `checkpoint.pth` file can be used to deploy the trained agent or for further analysis and experimentation, such as testing different evaluation scenarios or using the model as a baseline for more advanced reinforcement learning techniques.\n"
      ]
    }
  ]
}